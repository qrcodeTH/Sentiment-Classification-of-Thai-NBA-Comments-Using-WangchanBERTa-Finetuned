{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPLbEIgW6M0h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Dataset\n",
        "file_path = \"./nba-thaicomments-dataset/NBA_Comment_Data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",

        "df = df.rename(columns={'data': 'text', 'label': 'label'})\n",
        "\n",
        "# Map labels to integers if necessary\n",
        "label_map = {'P': 0, 'Neu': 1, 'Neg': 2}\n",
        "df['label'] = df['label'].map(label_map)\n",
        "\n",
        "# Convert the DataFrame to a Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "test_dataset = dataset_split['test']\n",
        "\n",
        "# Further split the training set into train and validation sets\n",
        "train_val_dataset = dataset_split['train'].train_test_split(test_size=0.1, seed=42)  # 10% of train for validation\n",
        "\n",
        "# Combine the splits into a DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_val_dataset['train'],\n",
        "    'validation': train_val_dataset['test'],\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Save the DatasetDict\n",
        "dataset_dict.save_to_disk('/kaggle/working/nba_comments_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"poom-sci/WangchanBERTa-finetuned-sentiment\")"
      ],
      "metadata": {
        "id": "K17Da6gV6SKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "wZT0rT2p6TcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = dataset_dict.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "FyQrsNny6UvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "UuTabIWe6U84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"poom-sci/WangchanBERTa-finetuned-sentiment\")"
      ],
      "metadata": {
        "id": "rzgdz5Yk6VHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "M_OAU3KE6VZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
